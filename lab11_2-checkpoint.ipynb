{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫번째 학습셋 입력:  유시민 검찰도 사법도 썩었지 vs 진중권 유시민 망상 대중은 현실로 믿어\n",
      "첫번째 테스트셋 입력:  박준영 “도자기 집에서 사용” 김선교 “궁궐 살았나”\n",
      "첫번째 학습셋 결과 one-hot 출력:  [1. 0. 0. 0.]\n",
      "첫번째 테스트셋 결과 one-hot 출력:  [1. 0. 0. 0.]\n",
      "전체 데이터셋 단어 토큰 개수:  23319\n",
      "첫번째 학습셋 토큰 결과:  [243, 7314, 7315, 7316, 122, 413, 243, 7317, 7318, 4360, 4361]\n",
      "첫번째 테스트셋 토큰 결과:  [3139, 22912, 1069, 22913, 22914, 22915, 22916]\n",
      "학습셋 제목 최대 길이:  18\n",
      "테스트셋 제목 최대 길이:  14\n",
      "학습셋 첫번째 패딩토큰:  [   0    0    0    0    0    0    0  243 7314 7315 7316  122  413  243\n",
      " 7317 7318 4360 4361]\n",
      "테스트셋 첫번쨰 패딩토큰:  [    0     0     0     0     0     0     0     0     0     0     0  3139\n",
      " 22912  1069 22913 22914 22915 22916]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 코드 내부에 한글을 사용가능 하게 해주는 부분입니다.\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# seed 값 설정\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "tf.random.set_seed(3)\n",
    "#파일 읽기\n",
    "train_docs = pd.read_csv('../study/dataset/train_SectionDataset.csv')\n",
    "test_docs = pd.read_csv('../study/dataset/test_SectionDataset.csv')\n",
    "\n",
    "x_train = train_docs.title\n",
    "x_test = test_docs.title\n",
    "X = x_train.append(x_test)\n",
    "print(\"첫번째 학습셋 입력: \", x_train[0])\n",
    "print(\"첫번째 테스트셋 입력: \", x_test[0])\n",
    "\n",
    "y_train = np_utils.to_categorical(train_docs.section)\n",
    "y_test = np_utils.to_categorical(test_docs.section)\n",
    "\n",
    "print(\"첫번째 학습셋 결과 one-hot 출력: \", y_train[0])\n",
    "print(\"첫번째 테스트셋 결과 one-hot 출력: \", y_test[0])\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(X)\n",
    "train_result = token.texts_to_sequences(x_train)\n",
    "test_result = token.texts_to_sequences(x_test)\n",
    "word_size = len(token.word_index)\n",
    "\n",
    "print(\"전체 데이터셋 단어 토큰 개수: \",word_size)\n",
    "print(\"첫번째 학습셋 토큰 결과: \", train_result[0])\n",
    "print(\"첫번째 테스트셋 토큰 결과: \", test_result[0])\n",
    "\n",
    "max = -1\n",
    "for i in range(10000):\n",
    "    result = text_to_word_sequence(train_docs.title[i]) \n",
    "    if(max < len(result)):\n",
    "        max = len(result)\n",
    "print(\"학습셋 제목 최대 길이: \",max)\n",
    "\n",
    "max2 = -1\n",
    "for i in range(150):\n",
    "    result2 = text_to_word_sequence(test_docs.title[i]) \n",
    "    if(max2 < len(result2)):\n",
    "        max2 = len(result2)\n",
    "        #print(result2)\n",
    "print(\"테스트셋 제목 최대 길이: \",max2)\n",
    "\n",
    "padded_train_x = pad_sequences(train_result, 18)\n",
    "print(\"학습셋 첫번째 패딩토큰: \",padded_train_x[0])\n",
    "padded_test_x = pad_sequences(test_result, 18)\n",
    "print(\"테스트셋 첫번쨰 패딩토큰: \",padded_test_x[0])\n",
    "\n",
    "# # 학습셋, 테스트셋 지정하기\n",
    "# (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=5000)\n",
    "\n",
    "# # 데이터 전처리\n",
    "# x_train = sequence.pad_sequences(x_train, maxlen=100)\n",
    "# x_test = sequence.pad_sequences(x_test, maxlen=100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 테스트 셋의 오차\n",
    "# y_vloss = history.history['val_loss']\n",
    "\n",
    "# # 학습셋의 오차\n",
    "# y_loss = history.history['loss']\n",
    "\n",
    "# # 그래프로 표현\n",
    "# x_len = numpy.arange(len(y_loss))\n",
    "# plt.plot(x_len, y_vloss, marker='.', c=\"red\", label='Testset_loss')\n",
    "# plt.plot(x_len, y_loss, marker='.', c=\"blue\", label='Trainset_loss')\n",
    "\n",
    "# # 그래프에 그리드를 주고 레이블을 표시\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.grid()\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('loss')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 18)          419760    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 18)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 64)          5824      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 55)                26400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 224       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 452,208\n",
      "Trainable params: 452,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yyk834\\Anaconda3\\envs\\tutorial\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 2s 224us/step - loss: 1.7729 - accuracy: 0.2705\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 2s 182us/step - loss: 1.3191 - accuracy: 0.4408\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 2s 180us/step - loss: 1.1256 - accuracy: 0.5440\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 2s 172us/step - loss: 0.9967 - accuracy: 0.5213\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 2s 180us/step - loss: 1.1298 - accuracy: 0.3277\n",
      "150/150 [==============================] - 0s 525us/step\n",
      "\n",
      " Test Accuracy: 0.3067\n"
     ]
    }
   ],
   "source": [
    "# 모델의 설정\n",
    "model = Sequential()\n",
    "model.add(Embedding(word_size+1, 18))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(64, 5, padding='valid', activation='relu',strides=1))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(55))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('relu'))\n",
    "model.summary()\n",
    "\n",
    "# 모델의 컴파일\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델의 실행\n",
    "history = model.fit(padded_train_x, y_train, batch_size=100, epochs=5)\n",
    "\n",
    "# 테스트 정확도 출력\n",
    "print(\"\\n Test Accuracy: %.4f\" % (model.evaluate(padded_test_x, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = token.texts_to_sequences(X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
